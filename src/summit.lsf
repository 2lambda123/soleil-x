#!/bin/bash -eu

# Create a directory on the scratch filesystem, for output (if the user hasn't
# specified an output directory explicitly).
OUT_DIR=
OUT_DIR_FOLLOWS=false
for ARG in $ARGS; do
    if [[ "$OUT_DIR_FOLLOWS" == true ]]; then
        OUT_DIR="$ARG"
        break
    elif [[ "$ARG" == "-o" ]]; then
        OUT_DIR_FOLLOWS=true
    fi
done
if [[ -z "$OUT_DIR" ]]; then
    OUT_DIR=/gpfs/alpinetds/proj-shared/csc275/stanford/"$LSB_JOBID"
    mkdir "$OUT_DIR"
    ARGS="$ARGS -o $OUT_DIR"
    echo "Redirecting output to $OUT_DIR"
fi

# Process CUDA configuration
GPU_OPTS=
if [[ "$USE_CUDA" == 1 ]]; then
    GPU_OPTS="-ll:gpu 3 -ll:fsize 1024 -ll:zsize 0 -ll:ib_zsize 0"
fi

# Make sure the number of requested ranks is divisible by 2.
NUM_RANKS="$(( NUM_RANKS + NUM_RANKS%2 ))"

# NOTE: We have to run through the pick_hcas.py script, due to a GASNet bug.
# NOTE: We have to disable MPI's CUDA hijack; it interferes with Realm's.
jsrun -n "$NUM_RANKS" -r 2 -a 1 -c 21 -g 3 -b rs \
    -E LD_LIBRARY_PATH -E SOLEIL_DIR -E REALM_BACKTRACE \
    --smpiargs="-x PAMI_DISABLE_CUDA_HOOK=1 -disable_gpu_hooks" \
    "$SOLEIL_DIR"/src/pick_hcas.py \
    "$EXECUTABLE" $ARGS \
    -ll:cpu 0 -ll:ocpu 1 -ll:onuma 0 -ll:okindhack -ll:othr 20 \
    $GPU_OPTS -ll:dma 2 -ll:ahandlers 2 \
    -ll:csize 16384 -ll:rsize 1024 -ll:gsize 0 -ll:ostack 8

# Resources:
# 2 NUMA domains per node
# 21 cores per NUMA domain
# 4-way SMT per core
# 256GB RAM per NUMA domain
# 3 Volta V100 GPUs per NUMA domain
# 16GB FB per GPU
